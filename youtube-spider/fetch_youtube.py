#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YouTube è§†é¢‘æŠ“å–å·¥å…·
ä½¿ç”¨ RSS feed è·å–æŒ‡å®š YouTube é¢‘é“çš„æœ€æ–°è§†é¢‘
"""

import json
import os
import re
from datetime import datetime
from urllib.parse import urlparse, parse_qs
import xml.etree.ElementTree as ET
import requests
from dateutil import parser as date_parser

class YouTubeSpider:
    def __init__(self, channel_handle="@saai-saai"):
        """
        åˆå§‹åŒ– YouTube çˆ¬è™«
        
        Args:
            channel_handle: YouTube é¢‘é“å¥æŸ„ï¼Œå¦‚ @saai-saai æˆ–é¢‘é“ID
        """
        self.channel_handle = channel_handle
        self.output_file = "youtube_videos.json"
        
        # ç§»é™¤ @ ç¬¦å·ï¼ˆå¦‚æœæœ‰ï¼‰
        if channel_handle.startswith('@'):
            self.channel_name = channel_handle[1:]
        else:
            self.channel_name = channel_handle
        
        # YouTube RSS feed URL
        # å¯¹äº @handle æ ¼å¼ï¼Œä½¿ç”¨ user å‚æ•°
        self.rss_url = f"https://www.youtube.com/feeds/videos.xml?user={self.channel_name}"
        
    def get_channel_id_from_handle(self):
        """
        ä» @handle è·å–é¢‘é“ID
        é€šè¿‡è®¿é—®é¢‘é“é¡µé¢å¹¶è§£æè·å–é¢‘é“ID
        """
        try:
            channel_url = f"https://www.youtube.com/@{self.channel_name}"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(channel_url, headers=headers, timeout=10)
            if response.status_code == 200:
                # å°è¯•ä»é¡µé¢ä¸­æå–é¢‘é“ID
                # YouTube é¡µé¢ä¸­åŒ…å« <link rel="canonical" href="https://www.youtube.com/channel/CHANNEL_ID">
                match = re.search(r'<link rel="canonical" href="https://www\.youtube\.com/channel/([^"]+)"', response.text)
                if match:
                    return match.group(1)
                
                # æˆ–è€…ä» meta æ ‡ç­¾ä¸­æå–
                match = re.search(r'"channelId":"([^"]+)"', response.text)
                if match:
                    return match.group(1)
        except Exception as e:
            print(f"è·å–é¢‘é“IDå¤±è´¥: {e}")
        
        return None
    
    def fetch_videos_from_rss(self, max_videos=10):
        """
        ä» RSS feed è·å–è§†é¢‘æ•°æ®
        
        Args:
            max_videos: æœ€å¤§è·å–è§†é¢‘æ•°é‡
        """
        videos = []
        
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨é¢‘é“IDè·å–RSS
            channel_id = self.get_channel_id_from_handle()
            if channel_id:
                rss_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
            else:
                # å¦‚æœæ— æ³•è·å–é¢‘é“IDï¼Œä½¿ç”¨ç”¨æˆ·å
                rss_url = self.rss_url
            
            print(f"æ­£åœ¨è·å– YouTube RSS feed: {rss_url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(rss_url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                # è§£æ XML
                root = ET.fromstring(response.content)
                
                # å‘½åç©ºé—´
                ns = {'atom': 'http://www.w3.org/2005/Atom',
                      'yt': 'http://www.youtube.com/xml/schemas/2015',
                      'media': 'http://search.yahoo.com/mrss/'}
                
                # è·å–æ‰€æœ‰ entryï¼ˆè§†é¢‘ï¼‰
                entries = root.findall('atom:entry', ns)
                
                for entry in entries[:max_videos]:
                    try:
                        # è§†é¢‘ID
                        video_id = entry.find('yt:videoId', ns)
                        video_id_text = video_id.text if video_id is not None else None
                        
                        # æ ‡é¢˜
                        title = entry.find('atom:title', ns)
                        title_text = title.text if title is not None else 'æ— æ ‡é¢˜'
                        
                        # é“¾æ¥
                        link = entry.find('atom:link', ns)
                        video_url = link.get('href') if link is not None else f"https://www.youtube.com/watch?v={video_id_text}"
                        
                        # å‘å¸ƒæ—¶é—´
                        published = entry.find('atom:published', ns)
                        published_text = published.text if published is not None else None
                        
                        # è§£æå‘å¸ƒæ—¶é—´
                        published_at = None
                        if published_text:
                            try:
                                published_at = date_parser.parse(published_text)
                            except:
                                published_at = datetime.now()
                        
                        # æè¿°
                        description = entry.find('atom:content', ns)
                        description_text = description.text if description is not None else ''
                        
                        # ç¼©ç•¥å›¾
                        thumbnail_url = None
                        media_group = entry.find('media:group', ns)
                        if media_group is not None:
                            thumbnail = media_group.find('media:thumbnail', ns)
                            if thumbnail is not None:
                                thumbnail_url = thumbnail.get('url')
                        
                        # ä½œè€…/é¢‘é“å
                        author = entry.find('atom:author', ns)
                        author_name = None
                        if author is not None:
                            name = author.find('atom:name', ns)
                            author_name = name.text if name is not None else None
                        
                        video_data = {
                            'video_id': video_id_text,
                            'title': title_text,
                            'url': video_url,
                            'published_at': published_at.isoformat() if published_at else datetime.now().isoformat(),
                            'description': description_text[:200] if description_text else '',  # é™åˆ¶æè¿°é•¿åº¦
                            'thumbnail_url': thumbnail_url,
                            'channel_name': author_name or self.channel_name,
                            'fetched_at': datetime.now().isoformat()
                        }
                        
                        videos.append(video_data)
                        print(f"å·²è·å–è§†é¢‘: {title_text[:50]}...")
                        
                    except Exception as e:
                        print(f"è§£æè§†é¢‘æ¡ç›®å¤±è´¥: {e}")
                        continue
                
                print(f"æˆåŠŸè·å– {len(videos)} ä¸ªè§†é¢‘")
                
            else:
                print(f"RSS feed è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return self.get_mock_data()
                
        except Exception as e:
            print(f"è·å– YouTube è§†é¢‘æ•°æ®å¤±è´¥: {e}")
            return self.get_mock_data()
        
        if not videos:
            print("æœªè·å–åˆ°ä»»ä½•è§†é¢‘ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®")
            return self.get_mock_data()
        
        return {
            'channel_handle': self.channel_handle,
            'channel_name': self.channel_name,
            'total_videos': len(videos),
            'fetched_at': datetime.now().isoformat(),
            'videos': videos
        }
    
    def get_mock_data(self):
        """è¿”å›æ¨¡æ‹Ÿæ•°æ®ï¼ˆå½“æ— æ³•è·å–çœŸå®æ•°æ®æ—¶ï¼‰"""
        print("ç”Ÿæˆæ¨¡æ‹Ÿ YouTube è§†é¢‘æ•°æ®...")
        
        return {
            'channel_handle': self.channel_handle,
            'channel_name': self.channel_name,
            'total_videos': 1,
            'fetched_at': datetime.now().isoformat(),
            'videos': [
                {
                    'video_id': 'example123',
                    'title': 'æœ€æ–°æŠ€æœ¯åˆ†äº«è§†é¢‘',
                    'url': 'https://www.youtube.com/watch?v=example123',
                    'published_at': datetime.now().isoformat(),
                    'description': 'è¿™æ˜¯ä¸€ä¸ªæŠ€æœ¯åˆ†äº«è§†é¢‘ï¼ŒåŒ…å«React 18æ–°ç‰¹æ€§è¯¦è§£å’Œå®æˆ˜é¡¹ç›®æ¼”ç¤ºã€‚',
                    'thumbnail_url': '',
                    'channel_name': self.channel_name,
                    'fetched_at': datetime.now().isoformat()
                }
            ]
        }
    
    def save_data(self, data):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        output_path = os.path.join(os.path.dirname(__file__), self.output_file)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def run(self, max_videos=10):
        """è¿è¡ŒæŠ“å–ä»»åŠ¡"""
        print(f"å¼€å§‹æŠ“å– YouTube é¢‘é“ {self.channel_handle} çš„è§†é¢‘...")
        print(f"ç›®æ ‡æŠ“å–æ•°é‡: {max_videos} ä¸ª")
        
        data = self.fetch_videos_from_rss(max_videos)
        
        if data and data.get('videos'):
            output_path = self.save_data(data)
            print(f"âœ… æŠ“å–æˆåŠŸï¼å…±è·å– {data['total_videos']} ä¸ªè§†é¢‘")
            print(f"ğŸ“ æ•°æ®æ–‡ä»¶: {output_path}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªè§†é¢‘çš„æ ‡é¢˜
            print("\nğŸ“º æŠ“å–åˆ°çš„è§†é¢‘:")
            for i, video in enumerate(data['videos'][:5], 1):
                print(f"  {i}. {video['title']}")
            if len(data['videos']) > 5:
                print(f"  ... è¿˜æœ‰ {len(data['videos']) - 5} ä¸ªè§†é¢‘")
        else:
            print("âŒ æœªèƒ½æŠ“å–åˆ°ä»»ä½•è§†é¢‘")
        
        return data

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    CHANNEL_HANDLE = "@saai-saai"  # YouTube é¢‘é“å¥æŸ„
    MAX_VIDEOS = 10  # æœ€å¤§æŠ“å–è§†é¢‘æ•°é‡
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶è¿è¡Œ
    spider = YouTubeSpider(CHANNEL_HANDLE)
    videos = spider.run(MAX_VIDEOS)
    
    return videos

if __name__ == "__main__":
    main()

